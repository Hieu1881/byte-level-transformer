each batch 
[ Someone is walking over there. ]
-> byte
[ 0,1,2,3,3,1,4,5,6,1,32,15,180 ]
-> patch
[ [0,1],[2,3,3,1,4],[5,6,1,32,15,180] ]
[ p1,    p2,         p3]

Local Encoder:

cross-attention first or last?? (first in Perceiver paper)
 
cross-att(byte, patch)  - only byte from the same patch can attend to each other.
self-att(patch)         - allow patch to attend to each other (causal or no mask??)

#Patch embedding initiated randomly or pooled from byte embedding???
All attention modules in the Perceiver are non-causal: we use no masks. (from Perceiver paper they have no mask? Be careful not to leak future token)



+Maybe should implement this in my own way but need to inspect their code thoroughly
+Do i need to use the hash n-gram embedding, can we use something else?

+What is the inference flow? 
patch input -> encoder -> global -> decoder -> new byte -> concat  (recalculate entropies and patch again for each new token)
                                                             |  
    |---------------------------------------------------------

+Can this architecture be cached with KV cache? The patch can change with each new byte generated (maybe this is not how blt generate new bytes)
==> Come up with a patching scheme that does not change the previous patch (so that we can do KV cache on the previous tokens)
This maybe difficult because if we keep the previous patches unchanged, we can only treat the new generated byte as an independent patch which 
is inefficient ==> Are there any ways to tackle this issue? Because without KV cache, the cost of inference with byte output would be too high.

Idea: Latent transformer will get patch as input and feed its patch embedding to local decoder. After generating a new byte, instead of feeding it into
local encoder and go through latent transformer once again (maybe that is not how their code works, check out this 
https://github.com/facebookresearch/blt/issues/30) the model will only go through encoder then decoder directly to decode the whole batch. In other words,
it will be like this:
byte -> encoder -> patch emb -> latent -> decoder -> next byte      this flow is for when entropy is high
           |                                 |
           byte emb -------------------------

byte -> encoder -> decoder -> next byte                this flow is for when entropy is low

==> Only invoke the latent transformer to decode new byte if entropy is high, otherwise bypass it 
  --> with this, only when a patch is finished then we have to invoke the latent transformer => patch unchanged => able to cache it
  --> reduce training and inference cost
 (They said they implement it this way https://github.com/facebookresearch/blt/issues/30, but cant find it in their code, maybe should inspect more 
 closely)

==> Need to come up with a new scheme to decide when to invoke the latent and when to keep generating for the same batch, when to create a new batch and
invoke the latent transformer